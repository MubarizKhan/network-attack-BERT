{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e269809d94c4f0281fb283b99ae7de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a44cde6922854fd8af9a73cc9118815b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2b2ba2604f884cf08278c1ccdb435477",
              "IPY_MODEL_48188d4075a746f49c606545c2e31764",
              "IPY_MODEL_9ca6114ff9974dd1a1a4457ac7fbffc7"
            ]
          }
        },
        "a44cde6922854fd8af9a73cc9118815b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b2ba2604f884cf08278c1ccdb435477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_754ef87d81c140bc8d9ca732f4660413",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d480ddb6823041b9be21b6267fb3009d"
          }
        },
        "48188d4075a746f49c606545c2e31764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2dd06b0f61a743d59156207800ff0eba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9a976841f9e4e0f8d0406824153c392"
          }
        },
        "9ca6114ff9974dd1a1a4457ac7fbffc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8414ead4f6ad4944919012cc404f74db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.09MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ca431554cf14dd3bfbce6931b52994d"
          }
        },
        "754ef87d81c140bc8d9ca732f4660413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d480ddb6823041b9be21b6267fb3009d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2dd06b0f61a743d59156207800ff0eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9a976841f9e4e0f8d0406824153c392": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8414ead4f6ad4944919012cc404f74db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ca431554cf14dd3bfbce6931b52994d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af5892e0f0cd4bd1af0b5cf8704e13ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_37c869d7100140c2a92eb3fa803790c2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b60477852cfe4a36b0446c09d8f894d7",
              "IPY_MODEL_73d0f42706a44e6c980c0546d805f2af",
              "IPY_MODEL_e194e49efe474541bbc6537b482f87b3"
            ]
          }
        },
        "37c869d7100140c2a92eb3fa803790c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b60477852cfe4a36b0446c09d8f894d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d6a600ed229e4e8595247a2f151a3576",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d73a44a15a54165ae92ee7b978967f1"
          }
        },
        "73d0f42706a44e6c980c0546d805f2af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_424ad39cceb64ff08c483d340a5f241e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ee39f4ab34442178885d5d1f30988ce"
          }
        },
        "e194e49efe474541bbc6537b482f87b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_36d160b86d5f4e5faa04c94d48ea3387",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 515B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49509048e4bf49dda385fb89075a8bdd"
          }
        },
        "d6a600ed229e4e8595247a2f151a3576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d73a44a15a54165ae92ee7b978967f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "424ad39cceb64ff08c483d340a5f241e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ee39f4ab34442178885d5d1f30988ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36d160b86d5f4e5faa04c94d48ea3387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49509048e4bf49dda385fb89075a8bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b1dfa736e0741d298743e9a737ca238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a28aa8aff3074dc995abc4f1ff9eee2c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a70b90a7dfc344898275ced1f4a569e8",
              "IPY_MODEL_669052fc7218490386912e57a941c924",
              "IPY_MODEL_f61dd4a9f312477581a58b6df61daef1"
            ]
          }
        },
        "a28aa8aff3074dc995abc4f1ff9eee2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a70b90a7dfc344898275ced1f4a569e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a294d222167b4c699e52efc4b604b39e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2784c6bdd52149a5899972bcc62e8a00"
          }
        },
        "669052fc7218490386912e57a941c924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_27202f926c184952ac1bc7e2f34df792",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6016a67663244d5ca2e6c4e1a5332f3a"
          }
        },
        "f61dd4a9f312477581a58b6df61daef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_95af6601bcf44bf58c4a51c5f7ffff84",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 1.37MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef751da1f09847a2b5998714d0894121"
          }
        },
        "a294d222167b4c699e52efc4b604b39e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2784c6bdd52149a5899972bcc62e8a00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "27202f926c184952ac1bc7e2f34df792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6016a67663244d5ca2e6c4e1a5332f3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95af6601bcf44bf58c4a51c5f7ffff84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef751da1f09847a2b5998714d0894121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a634e9f148ef4e7697537916ddbb3b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_27c09faafe23427e9397bead4fa6a9ab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_49ee673339f84bf7955f7ff517577845",
              "IPY_MODEL_2b5c7dddcda44e0099c5be49af2ca02b",
              "IPY_MODEL_484501a761cf4fd590ce28b13f445c82"
            ]
          }
        },
        "27c09faafe23427e9397bead4fa6a9ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49ee673339f84bf7955f7ff517577845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9e519451bed546b3932d9b0f16e279d3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8be9b48dcb0c42d4ac4b3bf10533297f"
          }
        },
        "2b5c7dddcda44e0099c5be49af2ca02b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_442ce971ede242b7810a348e06046a20",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41c69a2f50ae470f85750538c813302c"
          }
        },
        "484501a761cf4fd590ce28b13f445c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c561509e2ae24be69e4e035cdfb1d423",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 13.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_333ecbba953e45d788d4221423f80aad"
          }
        },
        "9e519451bed546b3932d9b0f16e279d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8be9b48dcb0c42d4ac4b3bf10533297f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "442ce971ede242b7810a348e06046a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41c69a2f50ae470f85750538c813302c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c561509e2ae24be69e4e035cdfb1d423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "333ecbba953e45d788d4221423f80aad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d8b14bb93614ee6a3e35b21201a253e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_236671f26bd44acab93e3175291e5364",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c084c099bc574d38b04b967c0bb56fed",
              "IPY_MODEL_d691d9ed1f0e4cb4aa5fd81114d26681",
              "IPY_MODEL_6492a779f7fb4c49970e844f4bcc0a5f"
            ]
          }
        },
        "236671f26bd44acab93e3175291e5364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c084c099bc574d38b04b967c0bb56fed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a447b2c412034e53867d524bd18b0644",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c771647dc094405add389ebf07c21b0"
          }
        },
        "d691d9ed1f0e4cb4aa5fd81114d26681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8744ee508d91447a8e42f64c4ed7b4b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2c6cf3b6e574f0abbed1580ccd13847"
          }
        },
        "6492a779f7fb4c49970e844f4bcc0a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_496f796e024d49158446cce0277e2720",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:14&lt;00:00, 33.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77e4f4931d214412bbdfad1485fd584e"
          }
        },
        "a447b2c412034e53867d524bd18b0644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c771647dc094405add389ebf07c21b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8744ee508d91447a8e42f64c4ed7b4b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2c6cf3b6e574f0abbed1580ccd13847": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "496f796e024d49158446cce0277e2720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77e4f4931d214412bbdfad1485fd584e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "njFbZgDT8jyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d279923-ae9c-41f5-a148-d8082b16f2a4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuhKbxNxTv9h"
      },
      "source": [
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWUzei1U8lXM"
      },
      "source": [
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46rj4ugo8lpI"
      },
      "source": [
        "filename = '/content/drive/MyDrive/MNAD-FYP/MADBERT/stringdata.h5'\n",
        "# filename = '/content/drive/MyDrive/MNAD-FYP/MADBERT/unsignedMNAD.h5'\n",
        "\n",
        "# filename = '/content/drive/MyDrive/MNAD-FYP/MachineLearningCVE/Dataset-Analysis/20khulkbenignTEST2.h5'\n",
        "# filename = '/content/drive/MyDrive/MNAD-FYP/MADBERT/100kMNAD.h5'\n",
        "\n",
        "\n",
        "# filename = '/content/drive/MyDrive/MNAD-FYP/MADBERT/300kMNAD.h5'\n",
        "\n",
        "\n",
        "# df.to_hdf(filename, 'data', mode='w', format='table')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96tmb_V08lsE",
        "outputId": "7ca3b912-5f49-4edc-b7ed-16b82049b78a"
      },
      "source": [
        "%time df = pd.read_hdf(filename, 'data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.02 s, sys: 855 ms, total: 3.87 s\n",
            "Wall time: 7.65 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj5Bbz-r8lu7",
        "outputId": "83ab01ba-9dd0-4c41-dbf6-8db4b88adf32"
      },
      "source": [
        "df.info()\n",
        "df['label'].value_counts()\n",
        "# .columns[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2827876 entries, 0 to 2827875\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Dtype \n",
            "---  ------       ----- \n",
            " 0   packet-data  object\n",
            " 1   label        int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 64.7+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     2271320\n",
              "4      230124\n",
              "10     158804\n",
              "2      128025\n",
              "3       10293\n",
              "7        7935\n",
              "11       5897\n",
              "6        5796\n",
              "5        5499\n",
              "1        1956\n",
              "12       1507\n",
              "14        652\n",
              "9          36\n",
              "13         21\n",
              "8          11\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "KeP4xc07ufX1",
        "outputId": "7c0b7135-23a1-4893-b90c-11f3960cc04b"
      },
      "source": [
        "df['label'].value_counts().plot(kind='bar')\n",
        "# plt.figure(figsize=(10, 5)) \n",
        "\n",
        "plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8], ['Benign ', 'Portscan', 'DDOS', 'DOS Hulk', 'FTP Patator', 'SSH Patator', \n",
        "                                   'Web Attacks', 'Brute Force ', ' XSS' ])\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYOnJLg8wOIm"
      },
      "source": [
        "# Creating the dataset\n",
        "# df = sns.load_dataset('titanic')\n",
        "df=df.groupby(' Label')[' Destination Port'].sum().to_frame().reset_index()\n",
        "\n",
        "#Creating the bar chart\n",
        "plt.barh(df[' Label'],df[' Destination Port'],color = ['#F0F8FF','#E6E6FA','#B0E0E6'])\n",
        "\n",
        "#Adding the aesthetics\n",
        "plt.title('Chart title')\n",
        "plt.xlabel('X axis title')\n",
        "plt.ylabel('Y axis title')\n",
        "\n",
        "#Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKVIavyYAoPP",
        "outputId": "e91c0976-8ce8-4ede-cfb0-fc49cb38252f"
      },
      "source": [
        "# df['label'] = df['label'].astype(str)\n",
        "type(df['label'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLDzqSacwNQi"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffFlvUli8lx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff1707d-c736-4a61-fa7e-ed284c86266a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 33.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 35.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 40.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK45z9zR8l0O"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cnjeFdx9ZEB"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV0zDEyE9ZM6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxgGI6OY9f2Z",
        "outputId": "06a7e586-7d8a-4d67-bb66-a0b41d4c1774"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqE5xB1o90VV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "2e269809d94c4f0281fb283b99ae7de4",
            "a44cde6922854fd8af9a73cc9118815b",
            "2b2ba2604f884cf08278c1ccdb435477",
            "48188d4075a746f49c606545c2e31764",
            "9ca6114ff9974dd1a1a4457ac7fbffc7",
            "754ef87d81c140bc8d9ca732f4660413",
            "d480ddb6823041b9be21b6267fb3009d",
            "2dd06b0f61a743d59156207800ff0eba",
            "a9a976841f9e4e0f8d0406824153c392",
            "8414ead4f6ad4944919012cc404f74db",
            "6ca431554cf14dd3bfbce6931b52994d",
            "af5892e0f0cd4bd1af0b5cf8704e13ce",
            "37c869d7100140c2a92eb3fa803790c2",
            "b60477852cfe4a36b0446c09d8f894d7",
            "73d0f42706a44e6c980c0546d805f2af",
            "e194e49efe474541bbc6537b482f87b3",
            "d6a600ed229e4e8595247a2f151a3576",
            "0d73a44a15a54165ae92ee7b978967f1",
            "424ad39cceb64ff08c483d340a5f241e",
            "0ee39f4ab34442178885d5d1f30988ce",
            "36d160b86d5f4e5faa04c94d48ea3387",
            "49509048e4bf49dda385fb89075a8bdd",
            "4b1dfa736e0741d298743e9a737ca238",
            "a28aa8aff3074dc995abc4f1ff9eee2c",
            "a70b90a7dfc344898275ced1f4a569e8",
            "669052fc7218490386912e57a941c924",
            "f61dd4a9f312477581a58b6df61daef1",
            "a294d222167b4c699e52efc4b604b39e",
            "2784c6bdd52149a5899972bcc62e8a00",
            "27202f926c184952ac1bc7e2f34df792",
            "6016a67663244d5ca2e6c4e1a5332f3a",
            "95af6601bcf44bf58c4a51c5f7ffff84",
            "ef751da1f09847a2b5998714d0894121",
            "a634e9f148ef4e7697537916ddbb3b21",
            "27c09faafe23427e9397bead4fa6a9ab",
            "49ee673339f84bf7955f7ff517577845",
            "2b5c7dddcda44e0099c5be49af2ca02b",
            "484501a761cf4fd590ce28b13f445c82",
            "9e519451bed546b3932d9b0f16e279d3",
            "8be9b48dcb0c42d4ac4b3bf10533297f",
            "442ce971ede242b7810a348e06046a20",
            "41c69a2f50ae470f85750538c813302c",
            "c561509e2ae24be69e4e035cdfb1d423",
            "333ecbba953e45d788d4221423f80aad"
          ]
        },
        "id": "hRydX7sa8l3C",
        "outputId": "b1c16c06-e611-4c1a-ba36-d0ac4db73689"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e269809d94c4f0281fb283b99ae7de4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af5892e0f0cd4bd1af0b5cf8704e13ce",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b1dfa736e0741d298743e9a737ca238",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a634e9f148ef4e7697537916ddbb3b21",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4672KtQmSwAN",
        "outputId": "1c3ffdfe-e1e4-4cda-87f3-04f3bfa77160"
      },
      "source": [
        "df.label.value_counts()\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 174883 entries, 0 to 174882\n",
            "Data columns (total 2 columns):\n",
            " #   Column       Non-Null Count   Dtype \n",
            "---  ------       --------------   ----- \n",
            " 0   packet-data  174883 non-null  object\n",
            " 1   label        174883 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 4.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFyC3zxD47FU"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# target = df['label']\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['label'])\n",
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veEBJpuI92vy"
      },
      "source": [
        "df.loc[df['label'] == 0].sample(5)[['packet-data', 'label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB1sS_ov93Ld"
      },
      "source": [
        "df.loc[df.label == 3].sample(5)[['packet-data', 'label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFvbyqZVJDjj"
      },
      "source": [
        "df.loc[df.label == 6].sample(5)[['packet-data', 'label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d__Ux2aW-UNZ",
        "outputId": "c55392ee-b31a-4ca7-b23c-d35b151627a1"
      },
      "source": [
        "packets = df['packet-data'].values\n",
        "labels = df['label'].values\n",
        "len(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "449499"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uajQnmMj93OT"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', packets[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(packets[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(packets[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktafb2hC93RS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6da8a2-0051-48ec-e549-d54cb71eded5"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for pack in packets:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(pack, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GMsMMkY93UL",
        "outputId": "5f9901d7-b57f-405c-c7a5-c05860f4eff1"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for pack in packets:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        pack,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwLMurZgCjPf",
        "outputId": "5f8ba2ce-598a-4177-9f1e-44538d708991"
      },
      "source": [
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', packets[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  213 0 0 0 0 0 1 0 0 0 0 0 0 24 226 0 0 0 0 0 0 0 3 0 0 0 0 0 0 56 1 0 0 0 0 0 0 1 0 0 6 0 0 0 0 0 0 0 0 0 85 0 0 0 0 \n",
            "Token IDs: tensor([  101, 19883,  1014,  1014,  1014,  1014,  1014,  1015,  1014,  1014,\n",
            "         1014,  1014,  1014,  1014,  2484, 21035,  1014,  1014,  1014,  1014,\n",
            "         1014,  1014,  1014,  1017,  1014,  1014,  1014,  1014,  1014,  1014,\n",
            "         5179,  1015,  1014,  1014,  1014,  1014,  1014,  1014,  1015,  1014,\n",
            "         1014,  1020,  1014,  1014,  1014,  1014,  1014,  1014,  1014,  1014,\n",
            "         1014,  5594,  1014,  1014,  1014,  1014,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By_2gnkx93W6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d599efd-bffe-4dd5-b392-48946070678a"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "314,649 training samples\n",
            "134,850 validation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuXRCl2YKf2b"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzkicPRtSm8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ddd9175-a523-4968-f291-3d5b89278021"
      },
      "source": [
        "len(train_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2972"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsCTp_mporB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3d8b14bb93614ee6a3e35b21201a253e",
            "236671f26bd44acab93e3175291e5364",
            "c084c099bc574d38b04b967c0bb56fed",
            "d691d9ed1f0e4cb4aa5fd81114d26681",
            "6492a779f7fb4c49970e844f4bcc0a5f",
            "a447b2c412034e53867d524bd18b0644",
            "5c771647dc094405add389ebf07c21b0",
            "8744ee508d91447a8e42f64c4ed7b4b5",
            "f2c6cf3b6e574f0abbed1580ccd13847",
            "496f796e024d49158446cce0277e2720",
            "77e4f4931d214412bbdfad1485fd584e"
          ]
        },
        "outputId": "4cac4ba6-6c73-4246-d039-821db2971fed"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 4, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d8b14bb93614ee6a3e35b21201a253e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9yaU0HlKgEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94042682-d9fe-448c-e36a-68d7daf0b27c"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (4, 768)\n",
            "classifier.bias                                                 (4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR6RAli6KgIC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7gHWCtDaBRb"
      },
      "source": [
        "# Create a callback that saves the model's weights\n",
        "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "#                                                  save_weights_only=True,\n",
        "#                                                  verbose=1)\n",
        "# os.listdir(checkpoint_dir)\n",
        "\n",
        "# import os\n",
        "\n",
        "# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "# output_dir = './model_save/'\n",
        "\n",
        "# # Create output directory if needed\n",
        "# if not os.path.exists(output_dir):\n",
        "#     os.makedirs(output_dir)\n",
        "\n",
        "# print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# # They can then be reloaded using `from_pretrained()`\n",
        "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "# model_to_save.save_pretrained(output_dir)\n",
        "# tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# # Good practice: save your training arguments together with the trained model\n",
        "# # torch.save(args, os.path.join(output_dir, 'training_args.bin')\n",
        "\n",
        "\n",
        "\n",
        "# EPOCH = 5\n",
        "# PATH = \"\"\n",
        "# LOSS = 0.4\n",
        "# PATH  = '/content/drive/MyDrive/MNAD-FYP/MADBERT/model.pt'\n",
        "\n",
        "# torch.save({\n",
        "#             'epoch': EPOCH,\n",
        "#             'model_state_dict': net.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'loss': LOSS,\n",
        "#             }, PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c87172-6c88-4314-d889-7c5e4df2dabe"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "        # function and pass down the arguments. The `forward` function is \n",
        "        # documented here: \n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "        # model.save\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of  9,833.    Elapsed: 0:00:26.\n",
            "  Batch    80  of  9,833.    Elapsed: 0:00:53.\n",
            "  Batch   120  of  9,833.    Elapsed: 0:01:19.\n",
            "  Batch   160  of  9,833.    Elapsed: 0:01:46.\n",
            "  Batch   200  of  9,833.    Elapsed: 0:02:13.\n",
            "  Batch   240  of  9,833.    Elapsed: 0:02:39.\n",
            "  Batch   280  of  9,833.    Elapsed: 0:03:06.\n",
            "  Batch   320  of  9,833.    Elapsed: 0:03:32.\n",
            "  Batch   360  of  9,833.    Elapsed: 0:03:59.\n",
            "  Batch   400  of  9,833.    Elapsed: 0:04:25.\n",
            "  Batch   440  of  9,833.    Elapsed: 0:04:52.\n",
            "  Batch   480  of  9,833.    Elapsed: 0:05:18.\n",
            "  Batch   520  of  9,833.    Elapsed: 0:05:45.\n",
            "  Batch   560  of  9,833.    Elapsed: 0:06:11.\n",
            "  Batch   600  of  9,833.    Elapsed: 0:06:37.\n",
            "  Batch   640  of  9,833.    Elapsed: 0:07:04.\n",
            "  Batch   680  of  9,833.    Elapsed: 0:07:30.\n",
            "  Batch   720  of  9,833.    Elapsed: 0:07:57.\n",
            "  Batch   760  of  9,833.    Elapsed: 0:08:23.\n",
            "  Batch   800  of  9,833.    Elapsed: 0:08:50.\n",
            "  Batch   840  of  9,833.    Elapsed: 0:09:16.\n",
            "  Batch   880  of  9,833.    Elapsed: 0:09:43.\n",
            "  Batch   920  of  9,833.    Elapsed: 0:10:09.\n",
            "  Batch   960  of  9,833.    Elapsed: 0:10:35.\n",
            "  Batch 1,000  of  9,833.    Elapsed: 0:11:02.\n",
            "  Batch 1,040  of  9,833.    Elapsed: 0:11:28.\n",
            "  Batch 1,080  of  9,833.    Elapsed: 0:11:55.\n",
            "  Batch 1,120  of  9,833.    Elapsed: 0:12:21.\n",
            "  Batch 1,160  of  9,833.    Elapsed: 0:12:47.\n",
            "  Batch 1,200  of  9,833.    Elapsed: 0:13:14.\n",
            "  Batch 1,240  of  9,833.    Elapsed: 0:13:40.\n",
            "  Batch 1,280  of  9,833.    Elapsed: 0:14:07.\n",
            "  Batch 1,320  of  9,833.    Elapsed: 0:14:33.\n",
            "  Batch 1,360  of  9,833.    Elapsed: 0:14:59.\n",
            "  Batch 1,400  of  9,833.    Elapsed: 0:15:26.\n",
            "  Batch 1,440  of  9,833.    Elapsed: 0:15:52.\n",
            "  Batch 1,480  of  9,833.    Elapsed: 0:16:19.\n",
            "  Batch 1,520  of  9,833.    Elapsed: 0:16:45.\n",
            "  Batch 1,560  of  9,833.    Elapsed: 0:17:12.\n",
            "  Batch 1,600  of  9,833.    Elapsed: 0:17:38.\n",
            "  Batch 1,640  of  9,833.    Elapsed: 0:18:04.\n",
            "  Batch 1,680  of  9,833.    Elapsed: 0:18:31.\n",
            "  Batch 1,720  of  9,833.    Elapsed: 0:18:57.\n",
            "  Batch 1,760  of  9,833.    Elapsed: 0:19:24.\n",
            "  Batch 1,800  of  9,833.    Elapsed: 0:19:50.\n",
            "  Batch 1,840  of  9,833.    Elapsed: 0:20:16.\n",
            "  Batch 1,880  of  9,833.    Elapsed: 0:20:43.\n",
            "  Batch 1,920  of  9,833.    Elapsed: 0:21:09.\n",
            "  Batch 1,960  of  9,833.    Elapsed: 0:21:36.\n",
            "  Batch 2,000  of  9,833.    Elapsed: 0:22:02.\n",
            "  Batch 2,040  of  9,833.    Elapsed: 0:22:29.\n",
            "  Batch 2,080  of  9,833.    Elapsed: 0:22:55.\n",
            "  Batch 2,120  of  9,833.    Elapsed: 0:23:21.\n",
            "  Batch 2,160  of  9,833.    Elapsed: 0:23:48.\n",
            "  Batch 2,200  of  9,833.    Elapsed: 0:24:14.\n",
            "  Batch 2,240  of  9,833.    Elapsed: 0:24:41.\n",
            "  Batch 2,280  of  9,833.    Elapsed: 0:25:07.\n",
            "  Batch 2,320  of  9,833.    Elapsed: 0:25:33.\n",
            "  Batch 2,360  of  9,833.    Elapsed: 0:26:00.\n",
            "  Batch 2,400  of  9,833.    Elapsed: 0:26:26.\n",
            "  Batch 2,440  of  9,833.    Elapsed: 0:26:53.\n",
            "  Batch 2,480  of  9,833.    Elapsed: 0:27:19.\n",
            "  Batch 2,520  of  9,833.    Elapsed: 0:27:45.\n",
            "  Batch 2,560  of  9,833.    Elapsed: 0:28:12.\n",
            "  Batch 2,600  of  9,833.    Elapsed: 0:28:38.\n",
            "  Batch 2,640  of  9,833.    Elapsed: 0:29:04.\n",
            "  Batch 2,680  of  9,833.    Elapsed: 0:29:31.\n",
            "  Batch 2,720  of  9,833.    Elapsed: 0:29:57.\n",
            "  Batch 2,760  of  9,833.    Elapsed: 0:30:24.\n",
            "  Batch 2,800  of  9,833.    Elapsed: 0:30:50.\n",
            "  Batch 2,840  of  9,833.    Elapsed: 0:31:16.\n",
            "  Batch 2,880  of  9,833.    Elapsed: 0:31:43.\n",
            "  Batch 2,920  of  9,833.    Elapsed: 0:32:09.\n",
            "  Batch 2,960  of  9,833.    Elapsed: 0:32:35.\n",
            "  Batch 3,000  of  9,833.    Elapsed: 0:33:02.\n",
            "  Batch 3,040  of  9,833.    Elapsed: 0:33:28.\n",
            "  Batch 3,080  of  9,833.    Elapsed: 0:33:55.\n",
            "  Batch 3,120  of  9,833.    Elapsed: 0:34:21.\n",
            "  Batch 3,160  of  9,833.    Elapsed: 0:34:47.\n",
            "  Batch 3,200  of  9,833.    Elapsed: 0:35:14.\n",
            "  Batch 3,240  of  9,833.    Elapsed: 0:35:40.\n",
            "  Batch 3,280  of  9,833.    Elapsed: 0:36:07.\n",
            "  Batch 3,320  of  9,833.    Elapsed: 0:36:33.\n",
            "  Batch 3,360  of  9,833.    Elapsed: 0:36:59.\n",
            "  Batch 3,400  of  9,833.    Elapsed: 0:37:26.\n",
            "  Batch 3,440  of  9,833.    Elapsed: 0:37:52.\n",
            "  Batch 3,480  of  9,833.    Elapsed: 0:38:18.\n",
            "  Batch 3,520  of  9,833.    Elapsed: 0:38:45.\n",
            "  Batch 3,560  of  9,833.    Elapsed: 0:39:11.\n",
            "  Batch 3,600  of  9,833.    Elapsed: 0:39:37.\n",
            "  Batch 3,640  of  9,833.    Elapsed: 0:40:04.\n",
            "  Batch 3,680  of  9,833.    Elapsed: 0:40:30.\n",
            "  Batch 3,720  of  9,833.    Elapsed: 0:40:57.\n",
            "  Batch 3,760  of  9,833.    Elapsed: 0:41:23.\n",
            "  Batch 3,800  of  9,833.    Elapsed: 0:41:49.\n",
            "  Batch 3,840  of  9,833.    Elapsed: 0:42:16.\n",
            "  Batch 3,880  of  9,833.    Elapsed: 0:42:42.\n",
            "  Batch 3,920  of  9,833.    Elapsed: 0:43:08.\n",
            "  Batch 3,960  of  9,833.    Elapsed: 0:43:35.\n",
            "  Batch 4,000  of  9,833.    Elapsed: 0:44:01.\n",
            "  Batch 4,040  of  9,833.    Elapsed: 0:44:27.\n",
            "  Batch 4,080  of  9,833.    Elapsed: 0:44:54.\n",
            "  Batch 4,120  of  9,833.    Elapsed: 0:45:20.\n",
            "  Batch 4,160  of  9,833.    Elapsed: 0:45:47.\n",
            "  Batch 4,200  of  9,833.    Elapsed: 0:46:13.\n",
            "  Batch 4,240  of  9,833.    Elapsed: 0:46:39.\n",
            "  Batch 4,280  of  9,833.    Elapsed: 0:47:06.\n",
            "  Batch 4,320  of  9,833.    Elapsed: 0:47:32.\n",
            "  Batch 4,360  of  9,833.    Elapsed: 0:47:58.\n",
            "  Batch 4,400  of  9,833.    Elapsed: 0:48:25.\n",
            "  Batch 4,440  of  9,833.    Elapsed: 0:48:51.\n",
            "  Batch 4,480  of  9,833.    Elapsed: 0:49:17.\n",
            "  Batch 4,520  of  9,833.    Elapsed: 0:49:44.\n",
            "  Batch 4,560  of  9,833.    Elapsed: 0:50:10.\n",
            "  Batch 4,600  of  9,833.    Elapsed: 0:50:36.\n",
            "  Batch 4,640  of  9,833.    Elapsed: 0:51:03.\n",
            "  Batch 4,680  of  9,833.    Elapsed: 0:51:29.\n",
            "  Batch 4,720  of  9,833.    Elapsed: 0:51:56.\n",
            "  Batch 4,760  of  9,833.    Elapsed: 0:52:22.\n",
            "  Batch 4,800  of  9,833.    Elapsed: 0:52:48.\n",
            "  Batch 4,840  of  9,833.    Elapsed: 0:53:15.\n",
            "  Batch 4,880  of  9,833.    Elapsed: 0:53:41.\n",
            "  Batch 4,920  of  9,833.    Elapsed: 0:54:07.\n",
            "  Batch 4,960  of  9,833.    Elapsed: 0:54:34.\n",
            "  Batch 5,000  of  9,833.    Elapsed: 0:55:00.\n",
            "  Batch 5,040  of  9,833.    Elapsed: 0:55:26.\n",
            "  Batch 5,080  of  9,833.    Elapsed: 0:55:53.\n",
            "  Batch 5,120  of  9,833.    Elapsed: 0:56:19.\n",
            "  Batch 5,160  of  9,833.    Elapsed: 0:56:45.\n",
            "  Batch 5,200  of  9,833.    Elapsed: 0:57:12.\n",
            "  Batch 5,240  of  9,833.    Elapsed: 0:57:38.\n",
            "  Batch 5,280  of  9,833.    Elapsed: 0:58:04.\n",
            "  Batch 5,320  of  9,833.    Elapsed: 0:58:31.\n",
            "  Batch 5,360  of  9,833.    Elapsed: 0:58:57.\n",
            "  Batch 5,400  of  9,833.    Elapsed: 0:59:24.\n",
            "  Batch 5,440  of  9,833.    Elapsed: 0:59:50.\n",
            "  Batch 5,480  of  9,833.    Elapsed: 1:00:16.\n",
            "  Batch 5,520  of  9,833.    Elapsed: 1:00:43.\n",
            "  Batch 5,560  of  9,833.    Elapsed: 1:01:09.\n",
            "  Batch 5,600  of  9,833.    Elapsed: 1:01:36.\n",
            "  Batch 5,640  of  9,833.    Elapsed: 1:02:02.\n",
            "  Batch 5,680  of  9,833.    Elapsed: 1:02:28.\n",
            "  Batch 5,720  of  9,833.    Elapsed: 1:02:55.\n",
            "  Batch 5,760  of  9,833.    Elapsed: 1:03:21.\n",
            "  Batch 5,800  of  9,833.    Elapsed: 1:03:48.\n",
            "  Batch 5,840  of  9,833.    Elapsed: 1:04:14.\n",
            "  Batch 5,880  of  9,833.    Elapsed: 1:04:40.\n",
            "  Batch 5,920  of  9,833.    Elapsed: 1:05:07.\n",
            "  Batch 5,960  of  9,833.    Elapsed: 1:05:33.\n",
            "  Batch 6,000  of  9,833.    Elapsed: 1:06:00.\n",
            "  Batch 6,040  of  9,833.    Elapsed: 1:06:26.\n",
            "  Batch 6,080  of  9,833.    Elapsed: 1:06:52.\n",
            "  Batch 6,120  of  9,833.    Elapsed: 1:07:19.\n",
            "  Batch 6,160  of  9,833.    Elapsed: 1:07:45.\n",
            "  Batch 6,200  of  9,833.    Elapsed: 1:08:11.\n",
            "  Batch 6,240  of  9,833.    Elapsed: 1:08:38.\n",
            "  Batch 6,280  of  9,833.    Elapsed: 1:09:04.\n",
            "  Batch 6,320  of  9,833.    Elapsed: 1:09:31.\n",
            "  Batch 6,360  of  9,833.    Elapsed: 1:09:57.\n",
            "  Batch 6,400  of  9,833.    Elapsed: 1:10:23.\n",
            "  Batch 6,440  of  9,833.    Elapsed: 1:10:50.\n",
            "  Batch 6,480  of  9,833.    Elapsed: 1:11:16.\n",
            "  Batch 6,520  of  9,833.    Elapsed: 1:11:42.\n",
            "  Batch 6,560  of  9,833.    Elapsed: 1:12:09.\n",
            "  Batch 6,600  of  9,833.    Elapsed: 1:12:35.\n",
            "  Batch 6,640  of  9,833.    Elapsed: 1:13:01.\n",
            "  Batch 6,680  of  9,833.    Elapsed: 1:13:28.\n",
            "  Batch 6,720  of  9,833.    Elapsed: 1:13:54.\n",
            "  Batch 6,760  of  9,833.    Elapsed: 1:14:21.\n",
            "  Batch 6,800  of  9,833.    Elapsed: 1:14:47.\n",
            "  Batch 6,840  of  9,833.    Elapsed: 1:15:13.\n",
            "  Batch 6,880  of  9,833.    Elapsed: 1:15:40.\n",
            "  Batch 6,920  of  9,833.    Elapsed: 1:16:06.\n",
            "  Batch 6,960  of  9,833.    Elapsed: 1:16:32.\n",
            "  Batch 7,000  of  9,833.    Elapsed: 1:16:59.\n",
            "  Batch 7,040  of  9,833.    Elapsed: 1:17:25.\n",
            "  Batch 7,080  of  9,833.    Elapsed: 1:17:51.\n",
            "  Batch 7,120  of  9,833.    Elapsed: 1:18:18.\n",
            "  Batch 7,160  of  9,833.    Elapsed: 1:18:44.\n",
            "  Batch 7,200  of  9,833.    Elapsed: 1:19:10.\n",
            "  Batch 7,240  of  9,833.    Elapsed: 1:19:37.\n",
            "  Batch 7,280  of  9,833.    Elapsed: 1:20:03.\n",
            "  Batch 7,320  of  9,833.    Elapsed: 1:20:30.\n",
            "  Batch 7,360  of  9,833.    Elapsed: 1:20:56.\n",
            "  Batch 7,400  of  9,833.    Elapsed: 1:21:22.\n",
            "  Batch 7,440  of  9,833.    Elapsed: 1:21:49.\n",
            "  Batch 7,480  of  9,833.    Elapsed: 1:22:15.\n",
            "  Batch 7,520  of  9,833.    Elapsed: 1:22:41.\n",
            "  Batch 7,560  of  9,833.    Elapsed: 1:23:08.\n",
            "  Batch 7,600  of  9,833.    Elapsed: 1:23:34.\n",
            "  Batch 7,640  of  9,833.    Elapsed: 1:24:01.\n",
            "  Batch 7,680  of  9,833.    Elapsed: 1:24:27.\n",
            "  Batch 7,720  of  9,833.    Elapsed: 1:24:53.\n",
            "  Batch 7,760  of  9,833.    Elapsed: 1:25:20.\n",
            "  Batch 7,800  of  9,833.    Elapsed: 1:25:46.\n",
            "  Batch 7,840  of  9,833.    Elapsed: 1:26:12.\n",
            "  Batch 7,880  of  9,833.    Elapsed: 1:26:39.\n",
            "  Batch 7,920  of  9,833.    Elapsed: 1:27:05.\n",
            "  Batch 7,960  of  9,833.    Elapsed: 1:27:32.\n",
            "  Batch 8,000  of  9,833.    Elapsed: 1:27:58.\n",
            "  Batch 8,040  of  9,833.    Elapsed: 1:28:24.\n",
            "  Batch 8,080  of  9,833.    Elapsed: 1:28:51.\n",
            "  Batch 8,120  of  9,833.    Elapsed: 1:29:17.\n",
            "  Batch 8,160  of  9,833.    Elapsed: 1:29:43.\n",
            "  Batch 8,200  of  9,833.    Elapsed: 1:30:10.\n",
            "  Batch 8,240  of  9,833.    Elapsed: 1:30:36.\n",
            "  Batch 8,280  of  9,833.    Elapsed: 1:31:02.\n",
            "  Batch 8,320  of  9,833.    Elapsed: 1:31:29.\n",
            "  Batch 8,360  of  9,833.    Elapsed: 1:31:55.\n",
            "  Batch 8,400  of  9,833.    Elapsed: 1:32:21.\n",
            "  Batch 8,440  of  9,833.    Elapsed: 1:32:48.\n",
            "  Batch 8,480  of  9,833.    Elapsed: 1:33:14.\n",
            "  Batch 8,520  of  9,833.    Elapsed: 1:33:40.\n",
            "  Batch 8,560  of  9,833.    Elapsed: 1:34:07.\n",
            "  Batch 8,600  of  9,833.    Elapsed: 1:34:33.\n",
            "  Batch 8,640  of  9,833.    Elapsed: 1:34:59.\n",
            "  Batch 8,680  of  9,833.    Elapsed: 1:35:26.\n",
            "  Batch 8,720  of  9,833.    Elapsed: 1:35:52.\n",
            "  Batch 8,760  of  9,833.    Elapsed: 1:36:18.\n",
            "  Batch 8,800  of  9,833.    Elapsed: 1:36:45.\n",
            "  Batch 8,840  of  9,833.    Elapsed: 1:37:11.\n",
            "  Batch 8,880  of  9,833.    Elapsed: 1:37:37.\n",
            "  Batch 8,920  of  9,833.    Elapsed: 1:38:04.\n",
            "  Batch 8,960  of  9,833.    Elapsed: 1:38:30.\n",
            "  Batch 9,000  of  9,833.    Elapsed: 1:38:57.\n",
            "  Batch 9,040  of  9,833.    Elapsed: 1:39:23.\n",
            "  Batch 9,080  of  9,833.    Elapsed: 1:39:49.\n",
            "  Batch 9,120  of  9,833.    Elapsed: 1:40:15.\n",
            "  Batch 9,160  of  9,833.    Elapsed: 1:40:41.\n",
            "  Batch 9,200  of  9,833.    Elapsed: 1:41:07.\n",
            "  Batch 9,240  of  9,833.    Elapsed: 1:41:34.\n",
            "  Batch 9,280  of  9,833.    Elapsed: 1:42:00.\n",
            "  Batch 9,320  of  9,833.    Elapsed: 1:42:26.\n",
            "  Batch 9,360  of  9,833.    Elapsed: 1:42:52.\n",
            "  Batch 9,400  of  9,833.    Elapsed: 1:43:18.\n",
            "  Batch 9,440  of  9,833.    Elapsed: 1:43:44.\n",
            "  Batch 9,480  of  9,833.    Elapsed: 1:44:11.\n",
            "  Batch 9,520  of  9,833.    Elapsed: 1:44:37.\n",
            "  Batch 9,560  of  9,833.    Elapsed: 1:45:03.\n",
            "  Batch 9,600  of  9,833.    Elapsed: 1:45:29.\n",
            "  Batch 9,640  of  9,833.    Elapsed: 1:45:55.\n",
            "  Batch 9,680  of  9,833.    Elapsed: 1:46:21.\n",
            "  Batch 9,720  of  9,833.    Elapsed: 1:46:48.\n",
            "  Batch 9,760  of  9,833.    Elapsed: 1:47:14.\n",
            "  Batch 9,800  of  9,833.    Elapsed: 1:47:40.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 1:48:01\n",
            "\n",
            "Running Validation...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O_NbXFGMukX"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWOrcqgSJ595"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZg4IbLSJ6Re"
      },
      "source": [
        "# filename = '/content/drive/MyDrive/MNAD-FYP/MADBERT/stringdata.h5'\n",
        "# filename = '/content/drive/MyDrive/MNAD-FYP/MADBERT/unsignedMNAD.h5'\n",
        "# filename = '/content/drive/MyDrive/MNAD-FYP/MachineLearningCVE/Dataset-Analysis/20khulkbenignTEST.h5'\n",
        "# filename = '/content/drive/MyDrive/MNAD-FYP/MachineLearningCVE/Dataset-Analysis/20khulkbenignV2Final.h5'\n",
        "filename = '/content/drive/MyDrive/MNAD-FYP/MachineLearningCVE/Dataset-Analysis/30kvalid.h5'\n",
        "# f = '/content/drive/MyDrive/MNAD-FYP/MachineLearningCVE/Dataset-Analysis/30kvalid.h5'\n",
        "\n",
        "\n",
        "\n",
        "# df.to_hdf(filename, 'data', mode='w', format='table')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv4Qr9B1J6Ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500f0863-f03e-4c88-cf88-c5cc1ba37a2a"
      },
      "source": [
        "%time df = pd.read_hdf(filename, 'data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 230 ms, sys: 82.2 ms, total: 313 ms\n",
            "Wall time: 1.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBCJV40qKSLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e08a52be-7b68-400a-d7ab-a6de2f5075fd"
      },
      "source": [
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    99710\n",
              "2    25000\n",
              "1    24999\n",
              "3    24910\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAN0LZBOOPVh"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "# df = pd.read_csv(filename) #header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "packets = df['packet-data'].values\n",
        "labels = df['label'].values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for pack in packets:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        pack,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTKPBUMFKpRC"
      },
      "source": [
        "# the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmZ93gNtQNwB"
      },
      "source": [
        "prediction_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFS1GnC1PsYI"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuNyiK0yPgj9"
      },
      "source": [
        "true_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWcy0X1hirdx"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1VRk9lzPVlv"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "# print(dtc.score(x_test, y_test))\n",
        "# print(accuracy_score(y_test, y_pred))\n",
        "# print(f1_score(y_test, y_pred, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaZQ4XC7kLs"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "f1 = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.\n",
        "\n",
        "  f11 = f1_score(true_labels[i], pred_labels_i, average='macro')\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i) \n",
        "\n",
        "  f1.append(f11)\n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyfY1tqxU0t9"
      },
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(f1))), y=f1, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCYZa1lQ8Jn8"
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "# f1 \n",
        "f1 =  f1_score(flat_true_labels, flat_predictions, average='macro')\n",
        "# print('Total f1: %.3f' % f1)\n",
        "print(f1)\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulTWaOr8QNY"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMzI3VTCZo5"
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUFUIQ8Cu8D"
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trr-A-POC18_"
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nskPzUM084zL"
      },
      "source": [
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = model_class.from_pretrained(output_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxSMw0FrptiL"
      },
      "source": [
        " # This code is taken from:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n",
        "\n",
        "# Don't apply weight decay to any parameters whose names include these tokens.\n",
        "# (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "# Separate the `weight` parameters from the `bias` parameters. \n",
        "# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
        "# - For the `bias` parameters, the 'weight_decay_rate' is 0.0. \n",
        "optimizer_grouped_parameters = [\n",
        "    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.1},\n",
        "    \n",
        "    # Filter for parameters which *do* include those.\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# Note - `optimizer_grouped_parameters` only includes the parameter values, not \n",
        "# the names."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC9aweHyLWYN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}